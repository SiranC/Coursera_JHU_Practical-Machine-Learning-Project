---
title: "Practical Machine Learning Project"
author: "SiranC"
output: 
        html_document:
                keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, you will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). (see the section on the Weight Lifting Exercise Dataset).

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

#### Data 

The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). The data for this project come from this [source](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).

## Data Process

First, download the datasets and read them in the memory.

```{r, echo=TRUE}
library(data.table)
# download the data
fileUrl_train = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
fileUrl_test = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
if (!file.exists('./pml-training.csv')){
        download.file(fileUrl_train,destfile = './pml-training.csv')
}
if (!file.exists('./pml-testing.csv')){
        download.file(fileUrl_test,destfile = './pml-testing.csv')
}
# read the data
training <- read.csv('pml-training.csv', header=TRUE)
testing <- read.csv('pml-testing.csv', header=TRUE)
```

The training data had 19622 observations and 160 variables. The distribution of five types of execution of the Unilateral Dumbbell Biceps Curl exercise, representing by A, B, C, D, and E, showed below:

```{r, echo=TRUE}
dim(training)
summary(training$classe)
```

Next, some columns have a lot of missing values, some are unrelated to predicting, and some are nearly zero variance, we will clean the datasets by removing those columns.

```{r, echo=TRUE}
library(caret)
#remove first 7 unrelated columns
training <- training[,8:length(training)]
#remove the columns with NAs and missing value
training <- training[,colSums(is.na(training))==0]
#remove the columns near zero variance
nzvCol <- nearZeroVar(training)
training <- training[,-nzvCol]
```

## Model Develop

Now we can select the model and train it.

### Cross-validation

We first split the training dataset into new train and validation sets.

```{r, echo=TRUE}
set.seed(123)
inTrain <- createDataPartition(y = training$classe,
                               p = 0.70, list = FALSE)
train <- training[inTrain,]
validation <- training[-inTrain,]
```

### Model Train

We will use the random forest algorithm to train the data since it is one of the most effective and accurate machine learning models. This ensemble technique uses bootstrap aggregation to sample the data with the replacement multiple times, then the decision trees will be generated in parallel from each subsample. The final predictions is based on combining decisions from submodels. Therefore, the random forest can reduce the bias since it not heavilly rely on specific features and it can prevent overfitting. 

```{r, echo=TRUE}
set.seed(123)
modFit <- train(classe~., data = train, method = 'rf')
```

### Model Validation

Let's use the validation dataset to test how our model performs by calculating the cross-validation accuracy.

```{r, echo=TRUE}
predVali <- predict(modFit, validation)
cfmVali <- confusionMatrix(predVali, validation$classe)
cfmVali
```

We can see that the accuracy is 0.9927 which is pretty good. Out-of-sample(or out-of-bag OOB) error rate can be seen in model **modFit** output **finalModel**. The OOB is 0.0066.

```{r, echo=TRUE}
modFit$finalModel
```

## Cases Prediction

Now we use out model to predict 20 test cases.

```{r, echo=TRUE}
predTest <- predict(modFit, testing[,8:length(testing)])
predTest
```